# -*- coding: utf-8 -*-
"""_Boston house price prediction_ .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1D1A8pD6jRYDPg1hlwVk6JvmBQMNvLrM-

import important libraries
"""

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')
from sklearn import preprocessing
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

"""load dataset"""

data_set=pd.read_csv('/content/Boston_House_ Dataset.csv')

data_set.head()

"""drop un important col"""

data_set.drop(columns=['Unnamed: 0'],axis=0,inplace=True)

data_set.head()

data_set.info()

"""preprocessing"""

data_set.isnull().sum()

#box_plots
fig,ax=plt.subplots(ncols=7,nrows=2,figsize=(20,10))
index=0
ax=ax.flatten()

for col,value in data_set.items():
  sns.boxplot(y=col,data=data_set,ax=ax[index])
  index+=1

plt.tight_layout(pad=0.5,w_pad=0.7,h_pad=5.0)

#dist_plots
fig,ax=plt.subplots(ncols=7,nrows=2,figsize=(20,10))
index=0
ax=ax.flatten()

for col,value in data_set.items():
  sns.distplot(value,ax=ax[index])
  index+=1

plt.tight_layout(pad=0.5,w_pad=0.7,h_pad=5.0)

#min_max_normalization
columns=['black','crim','zn','tax']
for column in columns:
  minn=min(data_set[column])
  maxx=max(data_set[column])
  data_set[column]=(data_set[column]-minn)/(maxx-minn)

#standarization
scaler=preprocessing.StandardScaler()

scaled_columns=scaler.fit_transform(data_set[columns])
scaled_columns=pd.DataFrame(scaled_columns,columns=columns)
scaled_columns.head()

#assign values to original dataset
for column in columns:
  data_set[column]=scaled_columns[column]

#co_relation matrix
co_rel=data_set.corr()
plt.figure(figsize=(20,10))
sns.heatmap(co_rel,annot=True,cmap='Spectral')

#plot relation between house price and istat
sns.regplot(y=data_set['medv'], x=data_set['rm'])

#plot relation between house price and istat
sns.regplot(y=data_set['medv'], x=data_set['lstat'])

"""split data"""

x=data_set.drop(columns=['medv','rad'],axis=1)
y=data_set['medv']

"""split our data"""

x_train,x_test,y_train,y_test=train_test_split(x,y,random_state=42)

"""train model"""

def train(model,x,y):
    model.fit(x_train,y_train)

    pre=model.predict(x_test)

    #cross_validation
    cv_score=cross_val_score(model,x,y,scoring='neg_mean_squared_error')
    cv_score=np.abs(np.mean(cv_score))

    print('mean squared error',mean_squared_error(y_test,pre))

    print('cv score =',cv_score)

# linear regression
from sklearn.linear_model import LinearRegression
model=LinearRegression()
train(model,x,y)
coef=pd.Series(model.coef_,x.columns).sort_values()
coef.plot(kind='bar',title='Model Coefficients')

# decision tree
from sklearn.tree import DecisionTreeRegressor
model=DecisionTreeRegressor()
train(model,x,y)
coef=pd.Series(model.feature_importances_,x.columns).sort_values(ascending=False)
coef.plot(kind='bar',title='Feature Importance')

# random forest
from sklearn.ensemble import RandomForestRegressor
model= RandomForestRegressor()
train(model,x,y)
coef=pd.Series(model.feature_importances_,x.columns).sort_values(ascending=False)
coef.plot(kind='bar',title='Feature Importance')

# extra tree
from sklearn.ensemble import ExtraTreesRegressor
model=ExtraTreesRegressor()
train(model,x,y)
coef=pd.Series(model.feature_importances_,x.columns).sort_values(ascending=False)
coef.plot(kind='bar',title='Feature Importance')